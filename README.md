# Home_Sales

This challenge was an exercise in utilizing SparkSQL to determine key data metrics. That data analyzed was collection of information gathered regarding house sales between the years of 2019 and 2022. Using SparkSQL, a temporary table was created prior to extracting specific metrics, such as the average price of of all the houses built each year. Thiis table was then cached, as well as partioned into a parquet, so that the total time it took took to determine the view per average house price exceeding $350,000 for each of the three could be compared. The cached temporary table took the least amount of time, taking a total of 0.3672 seconds to run, and the parquet table took the longest at 0.9817 seconds. After the tbales were compared, the temporary table was then uncached. The saved file was then downlodeded from Google Colab where it was run, and uploaded to this Github repository. 
